//! # CUDA GPU åŠ é€Ÿæ”¯æŒ (WIP)
//!
//! æä¾› NVIDIA GPU çš„ CUDA åŠ é€ŸåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
//! - è®¾å¤‡åˆå§‹åŒ–å’Œç®¡ç†
//! - å¼‚æ­¥å†…å­˜å¤åˆ¶
//! - JIT ç¼–è¯‘ GPU åŠ é€Ÿ
//! - è®¡ç®—å†…æ ¸æ‰§è¡Œ
//!
//! ## å½“å‰çŠ¶æ€
//!
//! - **å¼€å‘çŠ¶æ€**: ğŸš§ Work In Progress
//! - **åŠŸèƒ½å®Œæ•´æ€§**: ~60%ï¼ˆåŸºç¡€åŠŸèƒ½å·²å®ç°ï¼‰
//! - **ç”Ÿäº§å°±ç»ª**: âš ï¸ ä»…æ¨èç”¨äºå¼€å‘ç¯å¢ƒ
//!
//! ## å·²å®ç°åŠŸèƒ½
//!
//! - âœ… è®¾å¤‡åˆå§‹åŒ–å’ŒåŸºæœ¬ä¿¡æ¯è·å–
//! - âœ… å†…å­˜ç®¡ç†ï¼ˆmalloc/freeï¼‰
//! - âœ… å¼‚æ­¥å†…å­˜å¤åˆ¶ï¼ˆH2D/D2Hï¼‰
//! - âœ… æµç®¡ç†
//! - âœ… åŸºç¡€è®¾å¤‡ä¿¡æ¯æŸ¥è¯¢
//!
//! ## å¾…å®ç°åŠŸèƒ½
//!
//! - â³ è®¾å¤‡åˆ°è®¾å¤‡å†…å­˜å¤åˆ¶
//! - â³ å†…æ ¸æ‰§è¡Œé€»è¾‘
//! - â³ å¤šè®¾å¤‡ç®¡ç†
//! - â³ é«˜çº§CUDAç‰¹æ€§
//!
//! ## ä¾èµ–é¡¹
//!
//! - `cuda-rs`: CUDAé©±åŠ¨ç»‘å®š
//! - NVIDIA GPUé©±åŠ¨
//!
//! ## ç›¸å…³Issue
//!
//! - è·Ÿè¸ª: #å¾…åˆ›å»ºï¼ˆå†…æ ¸æ‰§è¡Œå®ç°ï¼‰
//!
//! ## è´¡çŒ®æŒ‡å—
//!
//! å¦‚æœæ‚¨æœ‰CUDAå¼€å‘ç»éªŒå¹¶å¸Œæœ›å¸®åŠ©å®ç°æ­¤æ¨¡å—ï¼Œè¯·ï¼š
//! 1. ç¡®ä¿æœ‰NVIDIA GPUå’ŒCUDAç¯å¢ƒ
//! 2. å‚è€ƒNVIDIA CUDAæ–‡æ¡£
//! 3. è”ç³»ç»´æŠ¤è€…review
//! 4. æäº¤PRå¹¶åŒ…å«æµ‹è¯•ç”¨ä¾‹

use std::ptr;
use std::sync::Arc;
use std::time::Instant;

use super::{PassthroughError, PciAddress};

// å¯¼å…¥vm-coreçš„GPUç±»å‹ä»¥å®ç°trait
#[cfg(feature = "cuda")]
use vm_core::gpu::{GpuBuffer, GpuCompute, GpuDeviceInfo, GpuExecutionResult, GpuKernel, GpuArg, GpuResult, GpuError};

/// CUDA è®¾å¤‡æŒ‡é’ˆ
#[derive(Debug, Clone, Copy)]
pub struct CudaDevicePtr {
    pub ptr: u64,
    pub size: usize,
}

unsafe impl Send for CudaDevicePtr {}
unsafe impl Sync for CudaDevicePtr {}

/// CUDA å†…å­˜å¤åˆ¶æ–¹å‘
#[derive(Debug, Clone, Copy)]
pub enum CudaMemcpyKind {
    HostToDevice,
    DeviceToHost,
    DeviceToDevice,
}

/// CUDA æµï¼ˆç”¨äºå¼‚æ­¥æ“ä½œï¼‰
pub struct CudaStream {
    pub stream: ptr::NonNull<std::ffi::c_void>,
}

unsafe impl Send for CudaStream {}
unsafe impl Sync for CudaStream {}

impl CudaStream {
    /// åˆ›å»ºæ–°çš„ CUDA æµ
    pub fn new() -> Result<Self, PassthroughError> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let mut stream = std::ptr::null_mut();
            unsafe {
                result::cuStreamCreate(&mut stream, 0).map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA stream create failed: {:?}",
                        e
                    ))
                })?;
            }

            Ok(Self {
                stream: ptr::NonNull::new(stream).expect("non-null stream"),
            })
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::warn!("CUDA support not enabled, creating mock stream");
            Ok(Self {
                stream: ptr::NonNull::dangling(),
            })
        }
    }

    /// åŒæ­¥æµ
    pub fn synchronize(&self) -> Result<(), PassthroughError> {
        #[cfg(feature = "cuda")]
        unsafe {
            use cudarc::driver::result;
            result::cuStreamSynchronize(self.stream.as_ptr()).map_err(|e| {
                PassthroughError::DriverBindingFailed(format!("CUDA stream sync failed: {:?}", e))
            })?;
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::warn!("CUDA synchronize called but CUDA not enabled");
        }

        Ok(())
    }
}

impl Drop for CudaStream {
    fn drop(&mut self) {
        #[cfg(feature = "cuda")]
        unsafe {
            use cudarc::driver::result;
            let _ = result::cuStreamDestroy_v2(self.stream.as_ptr());
        }
    }
}

/// CUDA åŠ é€Ÿå™¨
///
/// æä¾›åŸºæœ¬çš„ CUDA åŠ é€ŸåŠŸèƒ½ï¼ŒåŒ…æ‹¬å†…å­˜ç®¡ç†å’Œå†…æ ¸æ‰§è¡Œã€‚
pub struct CudaAccelerator {
    pub device_id: i32,
    pub device_name: String,
    pub compute_capability: (u32, u32),
    pub total_memory_mb: usize,
    pub stream: CudaStream,
}

impl CudaAccelerator {
    /// åˆ›å»ºæ–°çš„ CUDA åŠ é€Ÿå™¨
    ///
    /// # Arguments
    ///
    /// * `device_id` - CUDA è®¾å¤‡ IDï¼ˆé»˜è®¤ä¸º 0ï¼‰
    pub fn new(device_id: i32) -> Result<Self, PassthroughError> {
        log::info!("Initializing CUDA accelerator for device {}", device_id);

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            unsafe {
                // åˆå§‹åŒ– CUDA
                result::cuInit(0).map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!("CUDA init failed: {:?}", e))
                })?;

                // è·å–è®¾å¤‡
                let mut device = std::ptr::null_mut();
                result::cuDeviceGet(&mut device, device_id).map_err(|e| {
                    PassthroughError::DeviceNotFound(format!(
                        "CUDA device {} not found: {:?}",
                        device_id, e
                    ))
                })?;

                // è·å–è®¾å¤‡åç§°
                let mut name = [0u8; 256];
                result::cuDeviceGetName(name.as_mut_ptr() as *mut i8, 256, device).map_err(
                    |e| {
                        PassthroughError::DriverBindingFailed(format!(
                            "CUDA get name failed: {:?}",
                            e
                        ))
                    },
                )?;
                let device_name = String::from_utf8_lossy(&name)
                    .trim_end_matches('\0')
                    .to_string();

                // è·å–è®¡ç®—èƒ½åŠ›
                let mut major = 0u32;
                let mut minor = 0u32;
                result::cuDeviceComputeCapability(&mut major, &mut minor, device).map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA compute capability failed: {:?}",
                        e
                    ))
                })?;
                let compute_capability = (major, minor);

                // è·å–æ€»å†…å­˜
                let mut total_memory = 0usize;
                result::cuDeviceTotalMem_v2(&mut total_memory as *mut usize as *mut usize, device)
                    .map_err(|e| {
                        PassthroughError::DriverBindingFailed(format!(
                            "CUDA get memory failed: {:?}",
                            e
                        ))
                    })?;
                let total_memory_mb = total_memory / (1024 * 1024);

                let stream = CudaStream::new()?;

                log::info!(
                    "CUDA accelerator initialized: {} (Compute: {}.{} Memory: {} MB)",
                    device_name,
                    major,
                    minor,
                    total_memory_mb
                );

                Ok(Self {
                    device_id,
                    device_name,
                    compute_capability,
                    total_memory_mb,
                    stream,
                })
            }
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::warn!("CUDA support not enabled, creating mock accelerator");
            Ok(Self {
                device_id,
                device_name: "Mock CUDA Device".to_string(),
                compute_capability: (7, 5),
                total_memory_mb: 8192,
                stream: CudaStream::new()?,
            })
        }
    }

    /// åˆ†é… GPU å†…å­˜
    pub fn malloc(&self, size: usize) -> Result<CudaDevicePtr, PassthroughError> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let mut d_ptr = std::ptr::null_mut();
            unsafe {
                result::cuMemAlloc_v2(&mut d_ptr, size).map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!("CUDA malloc failed: {:?}", e))
                })?;
            }

            log::trace!("Allocated {} bytes on GPU", size);

            Ok(CudaDevicePtr {
                ptr: d_ptr as u64,
                size,
            })
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock CUDA malloc: {} bytes", size);
            Ok(CudaDevicePtr { ptr: 0, size })
        }
    }

    /// é‡Šæ”¾ GPU å†…å­˜
    pub fn free(&self, d_ptr: CudaDevicePtr) -> Result<(), PassthroughError> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            unsafe {
                result::cuMemFree_v2(d_ptr.ptr as *mut std::ffi::c_void).map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!("CUDA free failed: {:?}", e))
                })?;
            }

            log::trace!("Freed GPU memory at {:?}", d_ptr);
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock CUDA free");
        }

        Ok(())
    }

    /// å¼‚æ­¥å†…å­˜å¤åˆ¶ï¼ˆHost â†’ Deviceï¼‰
    pub async fn memcpy_h2d_async(
        &self,
        dst: CudaDevicePtr,
        src: &[u8],
    ) -> Result<(), PassthroughError> {
        let start = Instant::now();

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let size = std::cmp::min(src.len(), dst.size);
            unsafe {
                result::cuMemcpyHtoDAsync_v2(
                    dst.ptr as *mut std::ffi::c_void,
                    src.as_ptr() as *const std::ffi::c_void,
                    size,
                    self.stream.stream.as_ptr(),
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA H2D memcpy failed: {:?}",
                        e
                    ))
                })?;
            }

            log::trace!("Async memcpy H2D: {} bytes in {:?}", size, start.elapsed());
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock async memcpy H2D: {} bytes", src.len().min(dst.size));
        }

        Ok(())
    }

    /// å¼‚æ­¥å†…å­˜å¤åˆ¶ï¼ˆDevice â†’ Hostï¼‰
    pub async fn memcpy_d2h_async(
        &self,
        dst: &mut [u8],
        src: CudaDevicePtr,
    ) -> Result<(), PassthroughError> {
        let start = Instant::now();

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let size = std::cmp::min(dst.len(), src.size);
            unsafe {
                result::cuMemcpyDtoHAsync_v2(
                    dst.as_mut_ptr() as *mut std::ffi::c_void,
                    src.ptr as *const std::ffi::c_void,
                    size,
                    self.stream.stream.as_ptr(),
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA D2H memcpy failed: {:?}",
                        e
                    ))
                })?;
            }

            log::trace!("Async memcpy D2H: {} bytes in {:?}", size, start.elapsed());
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock async memcpy D2H: {} bytes", dst.len().min(src.size));
        }

        Ok(())
    }

    /// åŒæ­¥å†…å­˜å¤åˆ¶ï¼ˆHost â†” Deviceï¼‰
    pub fn memcpy_sync(
        &self,
        dst: CudaDevicePtr,
        src: &[u8],
        kind: CudaMemcpyKind,
    ) -> Result<(), PassthroughError> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let start = Instant::now();
            let size = std::cmp::min(src.len(), dst.size);

            match kind {
                CudaMemcpyKind::HostToDevice => unsafe {
                    result::cuMemcpyHtoD_v2(
                        dst.ptr as *mut std::ffi::c_void,
                        src.as_ptr() as *const std::ffi::c_void,
                        size,
                    )
                    .map_err(|e| {
                        PassthroughError::DriverBindingFailed(format!(
                            "CUDA memcpy H2D failed: {:?}",
                            e
                        ))
                    })?;
                },
                CudaMemcpyKind::DeviceToHost => unsafe {
                    result::cuMemcpyDtoH_v2(
                        dst.ptr as *mut std::ffi::c_void,
                        src.as_ptr() as *const std::ffi::c_void,
                        size,
                    )
                    .map_err(|e| {
                        PassthroughError::DriverBindingFailed(format!(
                            "CUDA memcpy D2H failed: {:?}",
                            e
                        ))
                    })?;
                },
                CudaMemcpyKind::DeviceToDevice => unsafe {
                    // æ³¨æ„: è¿™é‡Œdstå’Œsrcéƒ½åº”è¯¥è§£é‡Šä¸ºCudaDevicePtr
                    // ä½†å½“å‰APIç­¾åä½¿ç”¨src: &[u8]ï¼Œè¿™åœ¨DeviceToDeviceæƒ…å†µä¸‹ä¸å¤ªåˆé€‚
                    // è¿™æ˜¯ä¸€ä¸ªä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼Œæ›´å¥½çš„åšæ³•æ˜¯æ”¹å˜APIç­¾å
                    return Err(PassthroughError::DriverBindingFailed(
                        "Device-to-device memcpy requires special API. Use memcpy_d2d() instead.".to_string(),
                    ));
                }
            }

            log::trace!(
                "Sync memcpy {:?}: {} bytes in {:?}",
                kind,
                size,
                start.elapsed()
            );
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!(
                "Mock sync memcpy {:?}: {} bytes",
                kind,
                src.len().min(dst.size)
            );
        }

        Ok(())
    }

    /// è®¾å¤‡åˆ°è®¾å¤‡çš„å†…å­˜å¤åˆ¶
    ///
    /// åœ¨GPUè®¾å¤‡å†…å­˜ä¹‹é—´å¤åˆ¶æ•°æ®ï¼Œæ¯”Hostä¸­è½¬æ›´é«˜æ•ˆã€‚
    ///
    /// # Arguments
    ///
    /// * `dst` - ç›®æ ‡è®¾å¤‡æŒ‡é’ˆ
    /// * `src` - æºè®¾å¤‡æŒ‡é’ˆ
    /// * `size` - è¦å¤åˆ¶çš„å­—èŠ‚æ•°
    ///
    /// # Example
    ///
    /// ```ignore
    /// let accel = CudaAccelerator::new(0)?;
    /// let src = accel.malloc(1024)?;
    /// let dst = accel.malloc(1024)?;
    /// // ç›´æ¥åœ¨GPUå†…å­˜é—´å¤åˆ¶ï¼Œæ— éœ€Hostä¸­è½¬
    /// accel.memcpy_d2d(dst, src, 1024)?;
    /// ```
    pub fn memcpy_d2d(
        &self,
        dst: CudaDevicePtr,
        src: CudaDevicePtr,
        size: usize,
    ) -> Result<(), PassthroughError> {
        log::trace!("Device-to-device memcpy: {} bytes", size);

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let start = Instant::now();
            let copy_size = std::cmp::min(size, std::cmp::min(dst.size, src.size));

            unsafe {
                result::cuMemcpyDtoD_v2(
                    dst.ptr as *mut std::ffi::c_void,
                    src.ptr as *const std::ffi::c_void,
                    copy_size,
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA D2D memcpy failed: {:?}",
                        e
                    ))
                })?;
            }

            log::trace!("D2D memcpy: {} bytes in {:?}", copy_size, start.elapsed());
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock D2D memcpy: {} bytes", size);
        }

        Ok(())
    }

    /// å¼‚æ­¥è®¾å¤‡åˆ°è®¾å¤‡å†…å­˜å¤åˆ¶
    ///
    /// å¼‚æ­¥ç‰ˆæœ¬ï¼Œåœ¨æŒ‡å®šçš„CUDAæµä¸Šæ‰§è¡Œå¤åˆ¶æ“ä½œã€‚
    ///
    /// # Arguments
    ///
    /// * `dst` - ç›®æ ‡è®¾å¤‡æŒ‡é’ˆ
    /// * `src` - æºè®¾å¤‡æŒ‡é’ˆ
    /// * `size` - è¦å¤åˆ¶çš„å­—èŠ‚æ•°
    ///
    /// # Example
    ///
    /// ```ignore
    /// let accel = CudaAccelerator::new(0)?;
    /// let src = accel.malloc(1024)?;
    /// let dst = accel.memcpy_d2d_async(dst, src, 1024).await?;
    /// // ç­‰å¾…æ“ä½œå®Œæˆ
    /// accel.stream.synchronize()?;
    /// ```
    pub async fn memcpy_d2d_async(
        &self,
        dst: CudaDevicePtr,
        src: CudaDevicePtr,
        size: usize,
    ) -> Result<(), PassthroughError> {
        log::trace!("Async device-to-device memcpy: {} bytes", size);

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            let start = Instant::now();
            let copy_size = std::cmp::min(size, std::cmp::min(dst.size, src.size));

            unsafe {
                result::cuMemcpyDtoDAsync_v2(
                    dst.ptr as *mut std::ffi::c_void,
                    src.ptr as *const std::ffi::c_void,
                    copy_size,
                    self.stream.stream.as_ptr(),
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "CUDA async D2D memcpy failed: {:?}",
                        e
                    ))
                })?;
            }

            log::trace!(
                "Async D2D memcpy: {} bytes in {:?}",
                copy_size,
                start.elapsed()
            );
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::trace!("Mock async D2D memcpy: {} bytes", size);
        }

        Ok(())
    }

    /// è·å–è®¾å¤‡ä¿¡æ¯
    pub fn get_device_info(&self) -> CudaDeviceInfo {
        CudaDeviceInfo {
            device_id: self.device_id,
            name: self.device_name.clone(),
            compute_capability: self.compute_capability,
            total_memory_mb: self.total_memory_mb,
        }
    }

    /// æ£€æŸ¥è®¾å¤‡æ˜¯å¦æ”¯æŒæŸä¸ªåŠŸèƒ½
    pub fn supports_feature(&self, feature: CudaFeature) -> bool {
        match feature {
            CudaFeature::ComputeCapability(major, minor) => {
                self.compute_capability >= (major, minor)
            }
            CudaFeature::Memory(size_mb) => self.total_memory_mb >= size_mb,
        }
    }
}

/// CUDA è®¾å¤‡ä¿¡æ¯
#[derive(Debug, Clone)]
pub struct CudaDeviceInfo {
    pub device_id: i32,
    pub name: String,
    pub compute_capability: (u32, u32),
    pub total_memory_mb: usize,
}

/// CUDA åŠŸèƒ½ç‰¹æ€§
#[derive(Debug, Clone, Copy)]
pub enum CudaFeature {
    ComputeCapability(u32, u32),
    Memory(usize),
}

/// GPU è®¡ç®—å†…æ ¸ï¼ˆå ä½å®ç°ï¼‰
pub struct GpuKernel {
    pub name: String,
    pub kernel_ptr: u64,
}

impl GpuKernel {
    pub fn new(name: String) -> Self {
        Self {
            name,
            kernel_ptr: 0,
        }
    }

    /// æ‰§è¡Œå†…æ ¸
    ///
    /// ä½¿ç”¨ cuLaunchKernel API å¯åŠ¨ CUDA å†…æ ¸
    ///
    /// # Arguments
    ///
    /// * `grid_dim` - ç½‘æ ¼ç»´åº¦ (x, y, z)
    /// * `block_dim` - å—ç»´åº¦ (x, y, z)
    ///
    /// # Example
    ///
    /// ```ignore
    /// let kernel = GpuKernel::new("my_kernel".to_string());
    /// // å¯åŠ¨å†…æ ¸ï¼š1ä¸ªå—ï¼Œæ¯ä¸ªå—32ä¸ªçº¿ç¨‹
    /// kernel.launch((1, 1, 1), (32, 1, 1))?;
    /// ```
    pub fn launch(
        &self,
        grid_dim: (u32, u32, u32),
        block_dim: (u32, u32, u32),
    ) -> Result<(), PassthroughError> {
        log::debug!(
            "Launching GPU kernel '{}' with grid {:?} and block {:?}",
            self.name,
            grid_dim,
            block_dim
        );

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            // æ£€æŸ¥å†…æ ¸æ˜¯å¦å·²åŠ è½½
            if self.kernel_ptr == 0 {
                return Err(PassthroughError::DriverBindingFailed(
                    format!("Kernel '{}' not loaded. Call load_from_ptx() first.", self.name)
                ));
            }

            unsafe {
                // å¯åŠ¨å†…æ ¸
                // å‚æ•°è¯´æ˜:
                // - f: å†…æ ¸å‡½æ•°æŒ‡é’ˆ
                // - gridDimX/Y/Z: ç½‘æ ¼ç»´åº¦
                // - blockDimX/Y/Z: å—ç»´åº¦
                // - sharedMemBytes: å…±äº«å†…å­˜å¤§å° (bytes)
                // - hStream: CUDA æµ
                // - kernelParams: å†…æ ¸å‚æ•°æ•°ç»„
                // - extra: é¢å¤–å‚æ•°
                result::cuLaunchKernel(
                    self.kernel_ptr as *mut std::ffi::c_void,
                    grid_dim.0,
                    grid_dim.1,
                    grid_dim.2,
                    block_dim.0,
                    block_dim.1,
                    block_dim.2,
                    0, // sharedMemBytes - æš‚ä¸æ”¯æŒåŠ¨æ€å…±äº«å†…å­˜
                    std::ptr::null_mut(), // hStream - ä½¿ç”¨é»˜è®¤æµ
                    std::ptr::null_mut(), // kernelParams - æš‚ä¸æ”¯æŒå‚æ•°ä¼ é€’
                    std::ptr::null_mut(), // extra - æš‚ä¸æ”¯æŒé¢å¤–å‚æ•°
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "Failed to launch kernel '{}': {:?}",
                        self.name, e
                    ))
                })?;

                log::trace!(
                    "Kernel '{}' launched successfully (grid: {:?}, block: {:?})",
                    self.name,
                    grid_dim,
                    block_dim
                );
            }
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::warn!(
                "GPU kernel launch called but CUDA not enabled (kernel: '{}')",
                self.name
            );
        }

        Ok(())
    }

    /// ä» PTX (Parallel Thread Execution) ä»£ç åŠ è½½å†…æ ¸
    ///
    /// PTX æ˜¯ CUDA çš„æ±‡ç¼–è¯­è¨€ï¼Œéœ€è¦ä» PTX ä»£ç ä¸­åŠ è½½å†…æ ¸æ‰èƒ½æ‰§è¡Œã€‚
    ///
    /// # Arguments
    ///
    /// * `accelerator` - CUDA åŠ é€Ÿå™¨å¼•ç”¨
    /// * `ptx_code` - PTX ä»£ç å­—ç¬¦ä¸²
    /// * `kernel_name` - è¦åŠ è½½çš„å†…æ ¸åç§°
    ///
    /// # Example
    ///
    /// ```ignore
    /// let accelerator = CudaAccelerator::new(0)?;
    /// let mut kernel = GpuKernel::new("my_kernel".to_string());
    /// let ptx = r#"
    ///     .version 7.5
    ///     .target sm_50
    ///     .address_size 64
    ///
    ///     .visible .entry my_kernel(
    ///         .param .u64 .ptr .global .align 8 input
    ///     )
    ///     {
    ///         ret;
    ///     }
    /// "#;
    /// kernel.load_from_ptx(&accelerator, ptx, "my_kernel")?;
    /// ```
    pub fn load_from_ptx(
        &mut self,
        accelerator: &CudaAccelerator,
        ptx_code: &str,
        kernel_name: &str,
    ) -> Result<(), PassthroughError> {
        log::info!("Loading CUDA kernel '{}' from PTX", kernel_name);

        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;

            unsafe {
                // åŠ è½½ PTX æ¨¡å—
                let mut module = std::ptr::null_mut();
                result::cuModuleLoadData(
                    &mut module,
                    ptx_code.as_ptr() as *const std::ffi::c_void,
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "Failed to load PTX module for kernel '{}': {:?}",
                        kernel_name, e
                    ))
                })?;

                // è·å–å†…æ ¸å‡½æ•°æŒ‡é’ˆ
                let mut kernel_ptr = 0u64;
                result::cuModuleGetFunction(
                    &mut kernel_ptr as *mut u64 as *mut *mut std::ffi::c_void,
                    module,
                    std::ffi::CString::new(kernel_name)
                        .map_err(|e| {
                            PassthroughError::DriverBindingFailed(format!(
                                "Invalid kernel name '{}': {}",
                                kernel_name, e
                            ))
                        })?
                        .as_ptr(),
                )
                .map_err(|e| {
                    PassthroughError::DriverBindingFailed(format!(
                        "Failed to get kernel '{}' from module: {:?}",
                        kernel_name, e
                    ))
                })?;

                self.kernel_ptr = kernel_ptr;
                self.name = kernel_name.to_string();

                log::info!(
                    "Kernel '{}' loaded successfully (ptr: 0x{:x})",
                    kernel_name,
                    kernel_ptr
                );

                // æ³¨æ„: è¿™é‡Œä¸ç«‹å³å¸è½½æ¨¡å—ï¼Œå› ä¸ºå†…æ ¸éœ€è¦å®ƒ
                // åœ¨å®é™…ç”Ÿäº§ä»£ç ä¸­ï¼Œåº”è¯¥åœ¨ GpuKernel çš„ Drop ä¸­å¤„ç†æ¨¡å—å¸è½½
            }
        }

        #[cfg(not(feature = "cuda"))]
        {
            log::warn!(
                "load_from_ptx called but CUDA not enabled (kernel: '{}')",
                kernel_name
            );
        }

        Ok(())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cuda_accelerator_creation() {
        let accelerator = CudaAccelerator::new(0);
        assert!(accelerator.is_ok());

        let accel = accelerator.unwrap();
        assert_eq!(accel.device_id, 0);
        assert!(!accel.device_name.is_empty());
        assert!(accel.total_memory_mb > 0);
    }

    #[test]
    fn test_cuda_device_ptr() {
        let ptr = CudaDevicePtr {
            ptr: 0x1000,
            size: 1024,
        };
        assert_eq!(ptr.ptr, 0x1000);
        assert_eq!(ptr.size, 1024);
    }

    #[test]
    fn test_cuda_stream() {
        let stream = CudaStream::new();
        assert!(stream.is_ok());

        let stream = stream.unwrap();
        assert!(stream.synchronize().is_ok());
    }

    #[test]
    fn test_cuda_malloc_free() {
        let accelerator = CudaAccelerator::new(0).unwrap();
        let d_ptr = accelerator.malloc(4096);
        assert!(d_ptr.is_ok());

        let d_ptr = d_ptr.unwrap();
        assert_eq!(d_ptr.size, 4096);

        let result = accelerator.free(d_ptr);
        assert!(result.is_ok());
    }

    #[test]
    fn test_cuda_memcpy() {
        let accelerator = CudaAccelerator::new(0).unwrap();
        let d_ptr = accelerator.malloc(1024).unwrap();

        let src_data = vec![42u8; 1024];
        let result = accelerator.memcpy_sync(d_ptr, &src_data, CudaMemcpyKind::HostToDevice);
        assert!(result.is_ok());

        // æ¸…ç†
        let _ = accelerator.free(d_ptr);
    }

    #[test]
    fn test_cuda_feature_check() {
        let accelerator = CudaAccelerator::new(0).unwrap();

        // æµ‹è¯•è®¡ç®—èƒ½åŠ›æ£€æŸ¥
        assert!(accelerator.supports_feature(CudaFeature::ComputeCapability(5, 0)));
        assert!(!accelerator.supports_feature(CudaFeature::ComputeCapability(10, 0)));

        // æµ‹è¯•å†…å­˜æ£€æŸ¥
        assert!(accelerator.supports_feature(CudaFeature::Memory(100)));
        assert!(!accelerator.supports_feature(CudaFeature::Memory(100000)));
    }

    #[test]
    fn test_gpu_kernel() {
        let kernel = GpuKernel::new("test_kernel".to_string());
        assert_eq!(kernel.name, "test_kernel");

        // æµ‹è¯•å†…æ ¸å¯åŠ¨ï¼ˆåœ¨æœªåŠ è½½æ—¶åº”è¯¥å¤±è´¥ï¼‰
        let result = kernel.launch((1, 1, 1), (32, 1, 1));
        #[cfg(feature = "cuda")]
        assert!(result.is_err()); // å†…æ ¸æœªåŠ è½½ï¼Œåº”è¯¥å¤±è´¥
        #[cfg(not(feature = "cuda"))]
        assert!(result.is_ok()); // Mockæ¨¡å¼æ€»æ˜¯æˆåŠŸ
    }

    #[test]
    fn test_memcpy_d2d() {
        let accelerator = CudaAccelerator::new(0).unwrap();

        // åˆ†é…ä¸¤ä¸ªè®¾å¤‡å†…å­˜åŒºåŸŸ
        let src = accelerator.malloc(1024).unwrap();
        let dst = accelerator.malloc(1024).unwrap();

        // æµ‹è¯•è®¾å¤‡åˆ°è®¾å¤‡å¤åˆ¶
        let result = accelerator.memcpy_d2d(dst, src, 1024);
        assert!(result.is_ok());

        // æ¸…ç†
        let _ = accelerator.free(src);
        let _ = accelerator.free(dst);
    }

    #[test]
    fn test_cuda_device_info() {
        let accelerator = CudaAccelerator::new(0).unwrap();
        let info = accelerator.get_device_info();

        assert_eq!(info.device_id, 0);
        assert!(!info.name.is_empty());
        assert!(info.total_memory_mb > 0);

        // éªŒè¯è®¡ç®—èƒ½åŠ›æ ¼å¼åˆç†
        assert!(info.compute_capability.0 >= 5); // è‡³å°‘æ˜¯5.x
        assert!(info.compute_capability.0 <= 9); // ä¸è¶…è¿‡9.x (å½“å‰æœ€æ–°)
        assert!(info.compute_capability.1 <= 9);
    }
}

// ============================================================================
// GpuCompute trait å®ç°
// ============================================================================

#[cfg(feature = "cuda")]
impl GpuCompute for CudaAccelerator {
    fn initialize(&mut self) -> GpuResult<()> {
        // CudaAcceleratoråœ¨åˆ›å»ºæ—¶å·²ç»åˆå§‹åŒ–ï¼Œè¿™é‡Œåªéœ€ç¡®è®¤
        Ok(())
    }

    fn device_info(&self) -> GpuDeviceInfo {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;
            use cudarc::driver::sys;

            // Query actual free memory
            let free_memory_mb = unsafe {
                match result::cuMemGetInfo_v2() {
                    Ok((free, _total)) => free / (1024 * 1024),
                    Err(_) => self.total_memory_mb, // Fallback to total if query fails
                }
            };

            // Query multiprocessor count
            let multiprocessor_count = unsafe {
                result::cuDeviceGetAttribute(
                    self.device_id,
                    sys::CUdevice_attribute::CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT
                ).unwrap_or(0) as u32
            };

            // Query clock rate
            let clock_rate_khz = unsafe {
                result::cuDeviceGetAttribute(
                    self.device_id,
                    sys::CUdevice_attribute::CU_DEVICE_ATTRIBUTE_CLOCK_RATE
                ).unwrap_or(0) as u32
            };

            // Query L2 cache size
            let l2_cache_size = unsafe {
                result::cuDeviceGetAttribute(
                    self.device_id,
                    sys::CUdevice_attribute::CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE
                ).unwrap_or(0) as usize
            };

            // Detect unified memory support (CUDA 6.0+)
            let supports_unified_memory = self.compute_capability >= (5, 0);

            GpuDeviceInfo {
                device_id: self.device_id as u32,
                device_name: self.device_name.clone(),
                vendor: "NVIDIA".to_string(),
                total_memory_mb: self.total_memory_mb,
                free_memory_mb,
                multiprocessor_count,
                clock_rate_khz,
                l2_cache_size,
                supports_unified_memory,
                compute_capability: format!("{}.{}", self.compute_capability.0, self.compute_capability.1),
            }
        }

        #[cfg(not(feature = "cuda"))]
        {
            GpuDeviceInfo {
                device_id: self.device_id as u32,
                device_name: self.device_name.clone(),
                vendor: "NVIDIA".to_string(),
                total_memory_mb: self.total_memory_mb,
                free_memory_mb: self.total_memory_mb,
                multiprocessor_count: 0,
                clock_rate_khz: 0,
                l2_cache_size: 0,
                supports_unified_memory: false,
                compute_capability: format!("{}.{}", self.compute_capability.0, self.compute_capability.1),
            }
        }
    }

    fn allocate_memory(&self, size: usize) -> GpuResult<GpuBuffer> {
        let ptr = self.malloc(size)?;
        Ok(GpuBuffer {
            ptr: ptr.ptr,
            size: ptr.size,
        })
    }

    fn free_memory(&self, buffer: GpuBuffer) -> GpuResult<()> {
        let device_ptr = CudaDevicePtr {
            ptr: buffer.ptr,
            size: buffer.size,
        };
        self.free(device_ptr)?;
        Ok(())
    }

    fn copy_h2d(&self, host_data: &[u8], device_buffer: &GpuBuffer) -> GpuResult<()> {
        let device_ptr = CudaDevicePtr {
            ptr: device_buffer.ptr,
            size: device_buffer.size,
        };
        self.memcpy_h2d(device_ptr, host_data)?;
        Ok(())
    }

    fn copy_d2h(&self, device_buffer: &GpuBuffer, host_data: &mut [u8]) -> GpuResult<()> {
        let device_ptr = CudaDevicePtr {
            ptr: device_buffer.ptr,
            size: device_buffer.size,
        };
        self.memcpy_d2h(host_data, device_ptr)?;
        Ok(())
    }

    fn compile_kernel(&self, source: &str, kernel_name: &str) -> GpuResult<GpuKernel> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::nvrtc::result;

            // Create NVRTC program
            let program = unsafe {
                result::nvrtcCreateProgram(source.as_ptr() as *const i8, ptr::null(), 0, ptr::null(), ptr::null())
                    .map_err(|e| GpuError::CompilationFailed {
                        kernel: kernel_name.to_string(),
                        message: format!("Failed to create NVRTC program: {:?}", e),
                    })?
            };

            // Get compute capability for compilation options
            let compute_capability = format!("-arch=sm_{}", self.compute_capability.0 * 10 + self.compute_capability.1);

            // Compile the program
            let compilation_options = [compute_capability.as_str()];
            unsafe {
                result::nvrtcCompileProgram(program, compilation_options.len() as i32,
                    compilation_options.as_ptr() as *const *const i8)
                    .map_err(|e| {
                        // Get compilation log if available
                        let log_size = result::nvrtcGetProgramLogSize(program).unwrap_or(0);
                        if log_size > 0 {
                            let mut log = vec![0u8; log_size];
                            result::nvrtcGetProgramLog(program, log.as_mut_ptr()).ok();
                            let log_str = String::from_utf8_lossy(&log);
                            GpuError::CompilationFailed {
                                kernel: kernel_name.to_string(),
                                message: format!("NVRTC compilation failed: {:?}\nLog:\n{}", e, log_str),
                            }
                        } else {
                            GpuError::CompilationFailed {
                                kernel: kernel_name.to_string(),
                                message: format!("NVRTC compilation failed: {:?}", e),
                            }
                        }
                    })?;
            }

            // Get PTX size
            let ptx_size = unsafe {
                result::nvrtcGetPTXSize(program).map_err(|e| GpuError::CompilationFailed {
                    kernel: kernel_name.to_string(),
                    message: format!("Failed to get PTX size: {:?}", e),
                })?
            };

            // Get PTX code
            let mut ptx = vec![0u8; ptx_size];
            unsafe {
                result::nvrtcGetPTX(program, ptx.as_mut_ptr()).map_err(|e| GpuError::CompilationFailed {
                    kernel: kernel_name.to_string(),
                    message: format!("Failed to get PTX: {:?}", e),
                })?;
            }

            // Destroy the program
            unsafe {
                result::nvrtcDestroyProgram(&program).map_err(|e| {
                    log::warn!("Failed to destroy NVRTC program: {:?}", e);
                    // Non-fatal error, continue
                }).ok();
            }

            // Create kernel metadata
            let metadata = vm_core::gpu::KernelMetadata {
                name: kernel_name.to_string(),
                source: Some(source.to_string()),
                compiled_at: Some(std::time::SystemTime::now()
                    .duration_since(std::time::UNIX_EPOCH)
                    .unwrap()
                    .as_secs()),
                num_params: 0, // TODO: Parse from source
                shared_memory_size: 0, // TODO: Parse from source
            };

            Ok(GpuKernel {
                name: kernel_name.to_string(),
                binary: ptx,
                metadata,
            })
        }

        #[cfg(not(feature = "cuda"))]
        {
            Err(GpuError::CompilationFailed {
                kernel: kernel_name.to_string(),
                message: "CUDA feature not enabled".to_string(),
            })
        }
    }

    fn execute_kernel(
        &self,
        kernel: &GpuKernel,
        grid_dim: (u32, u32, u32),
        block_dim: (u32, u32, u32),
        args: &[GpuArg],
        shared_memory_size: usize,
    ) -> GpuResult<GpuExecutionResult> {
        #[cfg(feature = "cuda")]
        {
            use cudarc::driver::result;
            use std::ffi::CString;

            let start = std::time::Instant::now();

            // Load PTX module
            let ptx_cstring = CString::new(kernel.binary.clone()).map_err(|e| GpuError::ExecutionFailed {
                kernel: kernel.name.clone(),
                message: format!("Failed to create PTX CString: {}", e),
            })?;

            let mut module = std::ptr::null_mut();
            unsafe {
                result::cuModuleLoadData(&mut module, ptx_cstring.as_ptr() as *const _).map_err(|e| GpuError::ExecutionFailed {
                    kernel: kernel.name.clone(),
                    message: format!("Failed to load PTX module: {:?}", e),
                })?;
            }

            // Get kernel function
            let kernel_name_cstring = CString::new(kernel.name.as_str()).map_err(|e| GpuError::ExecutionFailed {
                kernel: kernel.name.clone(),
                message: format!("Failed to create kernel name CString: {}", e),
            })?;

            let mut kernel_func = std::ptr::null_mut();
            unsafe {
                result::cuModuleGetFunction(&mut kernel_func, module, kernel_name_cstring.as_ptr()).map_err(|e| {
                    // Cleanup module on error
                    result::cuModuleUnload(module).ok();
                    GpuError::ExecutionFailed {
                        kernel: kernel.name.clone(),
                        message: format!("Failed to get kernel function: {:?}", e),
                    }
                })?;
            }

            // Prepare kernel arguments
            // Convert GpuArg enum to raw pointers
            let mut kernel_args: Vec<Vec<u8>> = Vec::new();
            let mut kernel_arg_ptrs: Vec<*const std::ffi::c_void> = Vec::new();

            for arg in args {
                let arg_bytes = match arg {
                    GpuArg::U8(v) => v.to_le_bytes().to_vec(),
                    GpuArg::U32(v) => v.to_le_bytes().to_vec(),
                    GpuArg::U64(v) => v.to_le_bytes().to_vec(),
                    GpuArg::I32(v) => v.to_le_bytes().to_vec(),
                    GpuArg::I64(v) => v.to_le_bytes().to_vec(),
                    GpuArg::F32(v) => v.to_le_bytes().to_vec(),
                    GpuArg::F64(v) => v.to_le_bytes().to_vec(),
                    GpuArg::Buffer(buf) => buf.ptr.to_le_bytes().to_vec(),
                };
                kernel_args.push(arg_bytes);
                kernel_arg_ptrs.push(kernel_args.last().unwrap().as_ptr() as *const std::ffi::c_void);
            }

            // Launch kernel
            unsafe {
                result::cuLaunchKernel(
                    kernel_func,
                    grid_dim.0, grid_dim.1, grid_dim.2,  // grid dim
                    block_dim.0, block_dim.1, block_dim.2,  // block dim
                    shared_memory_size as u32,  // shared memory bytes
                    self.stream.stream,  // stream
                    kernel_arg_ptrs.as_mut_ptr() as *mut *mut _,  // kernel arguments
                    std::ptr::null_mut(),  // extra (optional)
                ).map_err(|e| {
                    // Cleanup on error
                    result::cuModuleUnload(module).ok();
                    GpuError::ExecutionFailed {
                        kernel: kernel.name.clone(),
                        message: format!("Failed to launch kernel: {:?}", e),
                    }
                })?;
            }

            // Cleanup module (kernel can still be used)
            unsafe {
                result::cuModuleUnload(module).map_err(|e| {
                    log::warn!("Failed to unload module: {:?}", e);
                }).ok();
            }

            let elapsed = start.elapsed();

            Ok(GpuExecutionResult {
                kernel_name: kernel.name.clone(),
                execution_time_us: elapsed.as_micros() as u64,
                bytes_transferred: 0, // TODO: Track actual memory transfers
            })
        }

        #[cfg(not(feature = "cuda"))]
        {
            Err(GpuError::ExecutionFailed {
                kernel: kernel.name.clone(),
                message: "CUDA feature not enabled".to_string(),
            })
        }
    }

    fn synchronize(&self) -> GpuResult<()> {
        Ok(())
    }
}
