name: Performance Benchmarks

on:
  push:
    branches: [main, master]
    paths:
      - 'vm-*/src/**'
      - 'benches/**'
      - '.github/workflows/benchmark.yml'
  pull_request:
    branches: [main, master]
    paths:
      - 'vm-*/src/**'
      - 'benches/**'
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  RUST_BACKTRACE: 1
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable
          components: rustfmt, clippy

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake g++ libssl-dev pkg-config

      - name: Install cargo-criterion
        run: cargo install cargo-criterion

      - name: Build release
        run: cargo build --release --all-features

      - name: Run all benchmarks
        run: |
          cargo bench --workspace --all-features -- --save-baseline main

      - name: Generate benchmark report
        run: |
          chmod +x scripts/run_benchmarks.sh
          ./scripts/run_benchmarks.sh

      - name: Detect regressions
        run: |
          chmod +x scripts/detect_regression.py
          python3 scripts/detect_regression.py
        continue-on-error: true

      - name: Store benchmark results
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'cargo'
          output-file-path: target/criterion/
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          alert-threshold: '200%'
          comment-on-alert: true
          fail-on-alert: true
          alert-comment-cc-users: '@maintainers'
        continue-on-error: true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            target/criterion/
            benches/baselines/
          retention-days: 30

      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ github.sha }}
          path: benchmark-report.md
          retention-days: 30

  compare:
    name: Compare Benchmarks
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          toolchain: stable

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
            target
          key: ${{ runner.os }}-cargo-bench-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-bench-

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake g++ libssl-dev pkg-config
          cargo install critcmp

      - name: Download previous benchmark results
        uses: actions/github-script@v6
        id: download
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');

            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'benchmark.yml',
              branch: context.repo.default_branch,
              status: 'success',
              per_page: 5
            });

            if (runs.data.workflow_runs.length > 0) {
              const runId = runs.data.workflow_runs[0].id;
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: runId
              });

              const benchmarkArtifact = artifacts.data.artifacts.find(
                a => a.name.startsWith('benchmark-results-')
              );

              if (benchmarkArtifact) {
                const download = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: benchmarkArtifact.id,
                  archive_format: 'zip'
                });

                fs.writeFileSync('benchmark.zip', Buffer.from(download.data));
                return runId;
              }
            }
            return null;

      - name: Extract previous benchmarks
        run: |
          if [ -f benchmark.zip ]; then
            unzip -q benchmark.zip -d target/criterion/previous || true
          fi

      - name: Run comparison
        run: |
          cargo bench --workspace --all-features -- --baseline main

      - name: Generate comparison report
        run: |
          python3 scripts/generate_benchmark_report.py --compare || true

      - name: Comment PR with results
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let comment = '## ðŸ“Š Performance Benchmark Results\n\n';

            try {
              const report = fs.readFileSync('benchmark-report.md', 'utf8');
              comment += report;
            } catch (e) {
              comment += 'Could not generate detailed benchmark report.\n\n';
              comment += 'Benchmark artifacts are available for download.\n';
            }

            comment += '\n---\n';
            comment += '*This comment was automatically generated by the benchmark workflow.*\n';

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment => {
              return comment.user.type === 'Bot' &&
                     comment.body.includes('ðŸ“Š Performance Benchmark Results');
            });

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  notify:
    name: Notify Results
    needs: [benchmark, compare]
    runs-on: ubuntu-latest
    if: always()

    steps:
      - name: Check benchmark status
        run: |
          if [ "${{ needs.benchmark.result }}" == "failure" ]; then
            echo "::error::Benchmarks failed"
            exit 1
          fi

      - name: Create summary
        run: |
          echo "## Benchmark Results Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- **Status**: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Comparison**: ${{ needs.compare.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full results available in the [artifacts](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})." >> $GITHUB_STEP_SUMMARY