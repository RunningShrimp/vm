name: Performance Monitoring

on:
  push:
    branches: [master, main]
    paths:
      - 'vm-*/src/**'
      - 'benches/**'
      - '.github/workflows/performance.yml'
      - 'Cargo.toml'
  pull_request:
    branches: [master, main]
    paths:
      - 'vm-*/src/**'
      - 'benches/**'
  schedule:
    # Run daily at 2 AM UTC to track performance over time
    - cron: '0 2 * * *'
  workflow_dispatch:

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1
  # Performance regression thresholds
  REGRESSION_THRESHOLD: "10"  # 10% regression threshold
  WARNING_THRESHOLD: "5"       # 5% warning threshold

jobs:
  # Run comprehensive benchmarks
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for comparison

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@nightly
        with:
          override: true

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-registry-

      - name: Cache cargo build
        uses: actions/cache@v4
        with:
          path: target
          key: ${{ runner.os }}-perf-bench-target-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-perf-bench-target-

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y \
            cmake \
            g++ \
            libssl-dev \
            pkg-config \
            libelf-dev \
            libz-dev \
            build-essential \
            jq

      - name: Install benchmark tools
        run: |
          cargo install critcmp

      - name: Build release optimized
        run: |
          cargo build --workspace --release --all-features

      - name: Run memory benchmarks
        run: |
          cargo bench --bench memory_optimization_benchmark -- --save-baseline main
        continue-on-error: true

      - name: Run JIT benchmarks
        run: |
          cargo bench --bench jit_compilation_bench -- --save-baseline main
        continue-on-error: true

      - name: Run async benchmarks
        run: |
          cargo bench --bench async_performance_benchmark -- --save-baseline main
        continue-on-error: true

      - name: Run comprehensive benchmarks
        run: |
          cargo bench --bench comprehensive_performance_benchmark -- --save-baseline main
        continue-on-error: true

      - name: Run TLB benchmarks
        run: |
          cargo bench --bench tlb_cache_benchmark -- --save-baseline main
        continue-on-error: true

      - name: Generate benchmark summary
        run: |
          chmod +x scripts/generate_benchmark_report.sh || echo "Script not found, skipping"
          ./scripts/generate_benchmark_report.sh || echo "Report generation failed, continuing..."
        continue-on-error: true

      - name: Store baseline results
        run: |
          mkdir -p benches/baselines
          cp -r target/criterion/* benches/baselines/ || true

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: |
            target/criterion/
            benches/baselines/
          retention-days: 30

      - name: Upload benchmark report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report-${{ github.sha }}
          path: benchmark-report.md
          retention-days: 30
        continue-on-error: true

  # Compare with previous benchmarks
  compare:
    name: Compare Performance
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    timeout-minutes: 20

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@nightly
        with:
          override: true

      - name: Cache cargo registry
        uses: actions/cache@v4
        with:
          path: |
            ~/.cargo/registry
            ~/.cargo/git
          key: ${{ runner.os }}-cargo-registry-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-cargo-registry-

      - name: Cache cargo build
        uses: actions/cache@v4
        with:
          path: target
          key: ${{ runner.os }}-perf-compare-target-${{ hashFiles('**/Cargo.lock') }}
          restore-keys: |
            ${{ runner.os }}-perf-compare-target-

      - name: Install dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y cmake g++ libssl-dev pkg-config jq
          cargo install critcmp

      - name: Download previous benchmark results
        uses: actions/github-script@v7
        id: download
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');

            // Find successful workflow runs on main branch
            const runs = await github.rest.actions.listWorkflowRuns({
              owner: context.repo.owner,
              repo: context.repo.repo,
              workflow_id: 'performance.yml',
              branch: context.repo.default_branch,
              status: 'success',
              per_page: 5
            });

            if (runs.data.workflow_runs.length > 0) {
              const runId = runs.data.workflow_runs[0].id;
              console.log(`Found previous run: ${runId}`);

              // Get artifacts from that run
              const artifacts = await github.rest.actions.listWorkflowRunArtifacts({
                owner: context.repo.owner,
                repo: context.repo.repo,
                run_id: runId
              });

              const benchmarkArtifact = artifacts.data.artifacts.find(
                a => a.name.startsWith('benchmark-results-')
              );

              if (benchmarkArtifact) {
                console.log(`Downloading artifact: ${benchmarkArtifact.name}`);

                const download = await github.rest.actions.downloadArtifact({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  artifact_id: benchmarkArtifact.id,
                  archive_format: 'zip'
                });

                fs.writeFileSync('previous-benchmark.zip', Buffer.from(download.data));
                return runId;
              }
            }
            return null;

      - name: Extract previous benchmarks
        run: |
          if [ -f previous-benchmark.zip ]; then
            unzip -q previous-benchmark.zip -d target/criterion/previous || true
            echo "Previous benchmarks extracted"
          else
            echo "No previous benchmark found"
          fi

      - name: Run current benchmarks
        run: |
          cargo build --workspace --release --all-features
          cargo bench --workspace --all-features -- --baseline previous

      - name: Compare with critcmp
        run: |
          if [ -f previous-benchmark.zip ]; then
            echo "## Performance Comparison" > comparison-report.md
            echo "" >> comparison-report.md
            critcmp previous main >> comparison-report.md 2>&1 || true
          else
            echo "No baseline available for comparison" > comparison-report.md
          fi

      - name: Detect regressions
        run: |
          chmod +x scripts/detect_regression.sh || echo "Script not found"
          ./scripts/detect_regression.sh --threshold ${{ env.REGRESSION_THRESHOLD }} || true
        continue-on-error: true

      - name: Generate PR comment
        run: |
          cat > pr-comment.md << 'EOF'
          ## ðŸ“Š Performance Benchmark Results

          ### Summary
          This PR's performance has been compared against the baseline from `${{ github.event.repository.default_branch }}`.

          ### Comparison Details
          $(cat comparison-report.md || echo "No comparison data available")

          ### Thresholds
          - ðŸŸ¢ **Improvement**: >${{ env.REGRESSION_THRESHOLD }}% faster
          - ðŸŸ¡ **Warning**: >${{ env.WARNING_THRESHOLD }}% slower
          - ðŸ”´ **Regression**: >${{ env.REGRESSION_THRESHOLD }}% slower

          ### Note
          Benchmarks are run with reduced sample size for faster CI feedback.
          Full benchmarks run on the main branch.

          ---
          *This comment was automatically generated by the performance monitoring workflow.*
          EOF

      - name: Comment on PR
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let comment;
            try {
              comment = fs.readFileSync('pr-comment.md', 'utf8');
            } catch (e) {
              comment = '## ðŸ“Š Performance Benchmark Results\n\nCould not generate detailed comparison report.';
            }

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(comment => {
              return comment.user.type === 'Bot' &&
                     comment.body.includes('ðŸ“Š Performance Benchmark Results');
            });

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: comment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: comment
              });
            }

  # Track performance trends
  trend-analysis:
    name: Performance Trend Analysis
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'schedule' || github.event_name == 'workflow_dispatch'
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          pattern: benchmark-results-*
          path: benchmark-results
          merge-multiple: true

      - name: Analyze trends
        run: |
          echo "# Performance Trend Analysis" >> trend-report.md
          echo "" >> trend-report.md
          echo "Generated at: $(date -u)" >> trend-report.md
          echo "" >> trend-report.md
          echo "## Benchmark Summary" >> trend-report.md
          echo "" >> trend-report.md

          # Analyze key benchmarks
          find benchmark-results -name "new" -type d | head -10 | while read dir; do
            if [ -f "$dir/estimates.json" ]; then
              bench_name=$(echo "$dir" | sed 's|.*/||')
              echo "### $bench_name" >> trend-report.md
              jq '.' "$dir/estimates.json" >> trend-report.md 2>/dev/null || true
              echo "" >> trend-report.md
            fi
          done

      - name: Upload trend report
        uses: actions/upload-artifact@v4
        with:
          name: trend-report-${{ github.sha }}
          path: trend-report.md
          retention-days: 90

  # Store performance data
  store-metrics:
    name: Store Performance Metrics
    needs: benchmark
    runs-on: ubuntu-latest
    if: github.event_name == 'push' && github.ref == 'refs/heads/master'
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download benchmark results
        uses: actions/download-artifact@v4
        with:
          name: benchmark-results-${{ github.sha }}
          path: target/criterion

      - name: Extract key metrics
        run: |
          mkdir -p perf-metrics

          # Extract memory performance
          find target/criterion -name "estimates.json" -exec sh -c '
            json_file="$1"
            bench_name=$(echo "$json_file" | sed "s|target/criterion/||" | sed "s|/new/estimates.json||" | sed "s|/estimates.json||")
            mean=$(jq -r ".mean.point_estimate" "$json_file" 2>/dev/null || echo "null")
            if [ "$mean" != "null" ]; then
              echo "$bench_name,$mean" >> perf-metrics/metrics.csv
            fi
          ' sh {} \;

          echo "benchmark,mean_time_ns" > perf-metrics/metrics.csv

      - name: Store metrics in repository
        run: |
          git config --local user.email "github-actions[bot]@users.noreply.github.com"
          git config --local user.name "github-actions[bot]"

          if [ ! -d .github/perf-data ]; then
            mkdir -p .github/perf-data
          fi

          cp -r perf-metrics/* .github/perf-data/ || true

          git add .github/perf-data/ || true
          git commit -m "chore: update performance metrics [skip ci]" || true
          git push || true

      - name: Create performance summary
        run: |
          echo "# Performance Metrics Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Key Metrics" >> $GITHUB_STEP_SUMMARY

          if [ -f perf-metrics/metrics.csv ]; then
            head -20 perf-metrics/metrics.csv >> $GITHUB_STEP_SUMMARY
          else
            echo "No metrics available" >> $GITHUB_STEP_SUMMARY
          fi

  # Final report
  performance-report:
    name: Performance Report
    runs-on: ubuntu-latest
    needs: [benchmark, compare, trend-analysis]
    if: always()

    steps:
      - name: Generate final report
        run: |
          echo "# Performance Monitoring Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Job Status" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "- Benchmark: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Comparison: ${{ needs.compare.result }}" >> $GITHUB_STEP_SUMMARY
          echo "- Trend Analysis: ${{ needs.trend-analysis.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Run ID**: ${{ github.run_id }}" >> $GITHUB_STEP_SUMMARY
          echo "**Commit**: ${{ github.sha }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Full benchmark results are available in the workflow artifacts" >> $GITHUB_STEP_SUMMARY
          echo "- Results are retained for 30 days" >> $GITHUB_STEP_SUMMARY

      - name: Check for failures
        if: needs.benchmark.result == 'failure'
        run: |
          echo "::error::Performance benchmarks failed. Please check the job logs for details."
          exit 1
