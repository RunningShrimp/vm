#!/usr/bin/env python3
"""
JITç¼–è¯‘å™¨å’ŒGCåŠŸèƒ½æµ‹è¯•æŠ¥å‘Šç”Ÿæˆè„šæœ¬

è¯¥è„šæœ¬ç”¨äºç”Ÿæˆvm-engine-jitæ¨¡å—çš„æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬ï¼š
- æµ‹è¯•è¦†ç›–ç‡ç»Ÿè®¡
- æ€§èƒ½åŸºå‡†æ•°æ®
- å‘ç°çš„é—®é¢˜å’Œæ”¹è¿›å»ºè®®
"""

import os
import sys
import subprocess
import json
import re
from datetime import datetime
from pathlib import Path

class TestReportGenerator:
    def __init__(self, project_root):
        self.project_root = Path(project_root)
        self.vm_engine_jit_dir = self.project_root / "vm-engine-jit"
        self.tests_dir = self.vm_engine_jit_dir / "tests"
        self.benches_dir = self.project_root / "benches"
        
        # æµ‹è¯•ç»“æœ
        self.test_results = {}
        self.coverage_data = {}
        self.benchmark_data = {}
        self.issues = []
        self.recommendations = []
        
    def run_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•å¹¶æ”¶é›†ç»“æœ"""
        print("ğŸ”§ è¿è¡ŒJITå’ŒGCæµ‹è¯•...")
        
        # è¿è¡Œå•å…ƒæµ‹è¯•
        self._run_unit_tests()
        
        # è¿è¡Œé›†æˆæµ‹è¯•
        self._run_integration_tests()
        
        # è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
        self._run_benchmarks()
        
        # ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š
        self._generate_coverage_report()
        
    def _run_unit_tests(self):
        """è¿è¡Œå•å…ƒæµ‹è¯•"""
        test_files = [
            "jit_optimizing_compiler_comprehensive_tests",
            "gc_comprehensive_tests", 
            "hotspot_cache_comprehensive_tests",
            "register_allocator_tests",
            "ewma_hotspot_tests",
            "gc_module_tests",
            "unified_cache_tests",
            "jit_error_tests"
        ]
        
        for test_file in test_files:
            print(f"  ğŸ“‹ è¿è¡Œ {test_file}...")
            try:
                result = subprocess.run(
                    ["cargo", "test", "--package", "vm-engine-jit", "--test", test_file],
                    cwd=self.vm_engine_jit_dir,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5åˆ†é’Ÿè¶…æ—¶
                )
                
                self.test_results[test_file] = {
                    "exit_code": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "success": result.returncode == 0
                }
                
                if result.returncode == 0:
                    print(f"    âœ… {test_file} é€šè¿‡")
                else:
                    print(f"    âŒ {test_file} å¤±è´¥")
                    self._extract_test_errors(test_file, result.stderr)
                    
            except subprocess.TimeoutExpired:
                print(f"    â° {test_file} è¶…æ—¶")
                self.test_results[test_file] = {
                    "exit_code": -1,
                    "stdout": "",
                    "stderr": "Test timed out",
                    "success": False
                }
                self.issues.append(f"{test_file}: æµ‹è¯•è¶…æ—¶")
            except Exception as e:
                print(f"    ğŸ’¥ {test_file} å¼‚å¸¸: {e}")
                self.test_results[test_file] = {
                    "exit_code": -2,
                    "stdout": "",
                    "stderr": str(e),
                    "success": False
                }
                self.issues.append(f"{test_file}: æ‰§è¡Œå¼‚å¸¸ - {e}")
    
    def _run_integration_tests(self):
        """è¿è¡Œé›†æˆæµ‹è¯•"""
        integration_test_files = [
            "jit_gc_integration_tests",
            "aot_integration_tests",
            "task3_integration"
        ]
        
        for test_file in integration_test_files:
            print(f"  ğŸ”— è¿è¡Œé›†æˆæµ‹è¯• {test_file}...")
            try:
                result = subprocess.run(
                    ["cargo", "test", "--package", "vm-engine-jit", "--test", test_file],
                    cwd=self.vm_engine_jit_dir,
                    capture_output=True,
                    text=True,
                    timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                )
                
                self.test_results[f"integration_{test_file}"] = {
                    "exit_code": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "success": result.returncode == 0
                }
                
                if result.returncode == 0:
                    print(f"    âœ… é›†æˆæµ‹è¯• {test_file} é€šè¿‡")
                else:
                    print(f"    âŒ é›†æˆæµ‹è¯• {test_file} å¤±è´¥")
                    self._extract_test_errors(f"integration_{test_file}", result.stderr)
                    
            except subprocess.TimeoutExpired:
                print(f"    â° é›†æˆæµ‹è¯• {test_file} è¶…æ—¶")
                self.issues.append(f"é›†æˆæµ‹è¯• {test_file}: æµ‹è¯•è¶…æ—¶")
            except Exception as e:
                print(f"    ğŸ’¥ é›†æˆæµ‹è¯• {test_file} å¼‚å¸¸: {e}")
                self.issues.append(f"é›†æˆæµ‹è¯• {test_file}: æ‰§è¡Œå¼‚å¸¸ - {e}")
    
    def _run_benchmarks(self):
        """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("  ğŸ“Š è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
        
        benchmark_names = [
            "jit_gc_performance_benchmarks"
        ]
        
        for benchmark in benchmark_names:
            print(f"    ğŸ“ˆ è¿è¡ŒåŸºå‡†æµ‹è¯• {benchmark}...")
            try:
                result = subprocess.run(
                    ["cargo", "bench", "--package", "vm-engine-jit", benchmark],
                    cwd=self.project_root,
                    capture_output=True,
                    text=True,
                    timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                )
                
                self.benchmark_data[benchmark] = {
                    "exit_code": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "success": result.returncode == 0
                }
                
                if result.returncode == 0:
                    print(f"    âœ… åŸºå‡†æµ‹è¯• {benchmark} å®Œæˆ")
                    self._extract_benchmark_results(benchmark, result.stdout)
                else:
                    print(f"    âŒ åŸºå‡†æµ‹è¯• {benchmark} å¤±è´¥")
                    self.issues.append(f"åŸºå‡†æµ‹è¯• {benchmark}: æ‰§è¡Œå¤±è´¥")
                    
            except subprocess.TimeoutExpired:
                print(f"    â° åŸºå‡†æµ‹è¯• {benchmark} è¶…æ—¶")
                self.issues.append(f"åŸºå‡†æµ‹è¯• {benchmark}: æµ‹è¯•è¶…æ—¶")
            except Exception as e:
                print(f"    ğŸ’¥ åŸºå‡†æµ‹è¯• {benchmark} å¼‚å¸¸: {e}")
                self.issues.append(f"åŸºå‡†æµ‹è¯• {benchmark}: æ‰§è¡Œå¼‚å¸¸ - {e}")
    
    def _generate_coverage_report(self):
        """ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Š"""
        print("  ğŸ“Š ç”Ÿæˆè¦†ç›–ç‡æŠ¥å‘Š...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº†cargo-tarpaulin
            result = subprocess.run(
                ["cargo", "tarpaulin", "--version"],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                print("    âš ï¸  cargo-tarpaulin æœªå®‰è£…ï¼Œè·³è¿‡è¦†ç›–ç‡æŠ¥å‘Š")
                return
                
            # è¿è¡Œè¦†ç›–ç‡æµ‹è¯•
            result = subprocess.run(
                [
                    "cargo", "tarpaulin",
                    "--out", "Html",
                    "--output-dir", "target/coverage",
                    "--package", "vm-engine-jit",
                    "test"
                ],
                cwd=self.vm_engine_jit_dir,
                capture_output=True,
                text=True,
                timeout=600
            )
            
            if result.returncode == 0:
                print("    âœ… è¦†ç›–ç‡æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
                self.coverage_data = {
                    "success": True,
                    "output": result.stdout,
                    "error": result.stderr
                }
            else:
                print("    âŒ è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                self.coverage_data = {
                    "success": False,
                    "output": result.stdout,
                    "error": result.stderr
                }
                self.issues.append("è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
                
        except Exception as e:
            print(f"    ğŸ’¥ è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸: {e}")
            self.issues.append(f"è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆå¼‚å¸¸ - {e}")
    
    def _extract_test_errors(self, test_name, stderr):
        """ä»æµ‹è¯•è¾“å‡ºä¸­æå–é”™è¯¯ä¿¡æ¯"""
        error_patterns = [
            r"thread '.*' panicked at '.*'",
            r"test .* failed",
            r"error: .*",
            r"panicked at '.*'"
        ]
        
        for pattern in error_patterns:
            matches = re.findall(pattern, stderr)
            for match in matches:
                self.issues.append(f"{test_name}: {match}")
    
    def _extract_benchmark_results(self, benchmark_name, stdout):
        """ä»åŸºå‡†æµ‹è¯•è¾“å‡ºä¸­æå–æ€§èƒ½æ•°æ®"""
        # å°è¯•æå–åŸºå‡†æµ‹è¯•ç»“æœ
        lines = stdout.split('\n')
        
        for line in lines:
            if "test result" in line.lower() or "benchmark" in line.lower():
                self.benchmark_data[benchmark_name]["results"] = line
                break
    
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("ğŸ“ ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š...")
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "project": "vm-engine-jit",
            "summary": self._generate_summary(),
            "test_results": self.test_results,
            "coverage": self.coverage_data,
            "benchmarks": self.benchmark_data,
            "issues": self.issues,
            "recommendations": self._generate_recommendations()
        }
        
        # ç”ŸæˆMarkdownæŠ¥å‘Š
        self._generate_markdown_report(report)
        
        # ç”ŸæˆJSONæŠ¥å‘Š
        self._generate_json_report(report)
        
        print("âœ… æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        return report
    
    def _generate_summary(self):
        """ç”Ÿæˆæµ‹è¯•æ‘˜è¦"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results.values() if result.get("success", False))
        failed_tests = total_tests - passed_tests
        
        return {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": (passed_tests / total_tests * 100) if total_tests > 0 else 0,
            "benchmarks_run": len(self.benchmark_data),
            "coverage_generated": self.coverage_data.get("success", False)
        }
    
    def _generate_recommendations(self):
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        failed_tests = [name for name, result in self.test_results.items() 
                      if not result.get("success", False)]
        
        if failed_tests:
            recommendations.append({
                "category": "æµ‹è¯•å¤±è´¥",
                "priority": "é«˜",
                "description": f"ä»¥ä¸‹æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤: {', '.join(failed_tests)}",
                "action": "æ£€æŸ¥å¤±è´¥çš„æµ‹è¯•ç”¨ä¾‹ï¼Œä¿®å¤ç›¸å…³ä»£ç é—®é¢˜"
            })
        
        # åŸºäºè¦†ç›–ç‡ç”Ÿæˆå»ºè®®
        if not self.coverage_data.get("success", False):
            recommendations.append({
                "category": "è¦†ç›–ç‡",
                "priority": "ä¸­",
                "description": "æµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Šç”Ÿæˆå¤±è´¥",
                "action": "å®‰è£…cargo-tarpaulin: cargo install cargo-tarpaulin"
            })
        
        # åŸºäºåŸºå‡†æµ‹è¯•ç”Ÿæˆå»ºè®®
        failed_benchmarks = [name for name, result in self.benchmark_data.items() 
                           if not result.get("success", False)]
        
        if failed_benchmarks:
            recommendations.append({
                "category": "æ€§èƒ½åŸºå‡†",
                "priority": "ä¸­",
                "description": f"ä»¥ä¸‹åŸºå‡†æµ‹è¯•å¤±è´¥: {', '.join(failed_benchmarks)}",
                "action": "æ£€æŸ¥åŸºå‡†æµ‹è¯•ç¯å¢ƒï¼Œç¡®ä¿ç³»ç»Ÿèµ„æºå……è¶³"
            })
        
        # é€šç”¨å»ºè®®
        if len(self.issues) > 5:
            recommendations.append({
                "category": "æ•´ä½“è´¨é‡",
                "priority": "é«˜",
                "description": f"å‘ç° {len(self.issues)} ä¸ªé—®é¢˜ï¼Œéœ€è¦ç³»ç»Ÿæ€§æ”¹è¿›",
                "action": "å»ºè®®è¿›è¡Œä»£ç å®¡æŸ¥ï¼Œæ”¹è¿›æµ‹è¯•ç­–ç•¥"
            })
        
        return recommendations
    
    def _generate_markdown_report(self, report):
        """ç”ŸæˆMarkdownæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        report_content = f"""# JITç¼–è¯‘å™¨å’ŒGCåŠŸèƒ½æµ‹è¯•æŠ¥å‘Š

## æµ‹è¯•æ¦‚è§ˆ

- **æµ‹è¯•æ—¶é—´**: {report['timestamp']}
- **é¡¹ç›®**: {report['project']}
- **æ€»æµ‹è¯•æ•°**: {report['summary']['total_tests']}
- **é€šè¿‡æµ‹è¯•æ•°**: {report['summary']['passed_tests']}
- **å¤±è´¥æµ‹è¯•æ•°**: {report['summary']['failed_tests']}
- **æˆåŠŸç‡**: {report['summary']['success_rate']:.1f}%
- **åŸºå‡†æµ‹è¯•æ•°**: {report['summary']['benchmarks_run']}
- **è¦†ç›–ç‡æŠ¥å‘Š**: {'âœ… å·²ç”Ÿæˆ' if report['summary']['coverage_generated'] else 'âŒ æœªç”Ÿæˆ'}

## æµ‹è¯•ç»“æœè¯¦æƒ…

"""
        
        # æ·»åŠ æµ‹è¯•ç»“æœ
        for test_name, result in report['test_results'].items():
            status = "âœ… é€šè¿‡" if result.get('success', False) else "âŒ å¤±è´¥"
            report_content += f"### {test_name}\n\n**çŠ¶æ€**: {status}\n\n"
            
            if not result.get('success', False) and result.get('stderr'):
                report_content += f"**é”™è¯¯ä¿¡æ¯**:\n```\n{result['stderr']}\n```\n\n"
        
        # æ·»åŠ åŸºå‡†æµ‹è¯•ç»“æœ
        if report['benchmarks']:
            report_content += "## æ€§èƒ½åŸºå‡†æµ‹è¯•\n\n"
            for benchmark_name, result in report['benchmarks'].items():
                status = "âœ… å®Œæˆ" if result.get('success', False) else "âŒ å¤±è´¥"
                report_content += f"### {benchmark_name}\n\n**çŠ¶æ€**: {status}\n\n"
                
                if result.get('results'):
                    report_content += f"**ç»“æœ**: {result['results']}\n\n"
        
        # æ·»åŠ é—®é¢˜åˆ—è¡¨
        if report['issues']:
            report_content += "## å‘ç°çš„é—®é¢˜\n\n"
            for i, issue in enumerate(report['issues'], 1):
                report_content += f"{i}. {issue}\n"
            report_content += "\n"
        
        # æ·»åŠ æ”¹è¿›å»ºè®®
        if report['recommendations']:
            report_content += "## æ”¹è¿›å»ºè®®\n\n"
            for rec in report['recommendations']:
                priority_emoji = "ğŸ”´" if rec['priority'] == 'é«˜' else "ğŸŸ¡" if rec['priority'] == 'ä¸­' else "ğŸŸ¢"
                report_content += f"### {priority_emoji} {rec['category']}\n\n"
                report_content += f"**æè¿°**: {rec['description']}\n\n"
                report_content += f"**å»ºè®®æ“ä½œ**: {rec['action']}\n\n"
        
        # å†™å…¥æŠ¥å‘Šæ–‡ä»¶
        report_path = self.project_root / "vm_engine_jit_test_report.md"
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"ğŸ“„ MarkdownæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    
    def _generate_json_report(self, report):
        """ç”ŸæˆJSONæ ¼å¼çš„æµ‹è¯•æŠ¥å‘Š"""
        report_path = self.project_root / "vm_engine_jit_test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ JSONæŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) != 2:
        print("ç”¨æ³•: python generate_jit_gc_test_report.py <é¡¹ç›®æ ¹ç›®å½•>")
        sys.exit(1)
    
    project_root = sys.argv[1]
    
    if not os.path.exists(project_root):
        print(f"é”™è¯¯: é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {project_root}")
        sys.exit(1)
    
    # åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
    generator = TestReportGenerator(project_root)
    
    # è¿è¡Œæµ‹è¯•
    generator.run_tests()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = generator.generate_report()
    
    # æ‰“å°æ‘˜è¦
    summary = report['summary']
    print(f"\nğŸ“Š æµ‹è¯•æ‘˜è¦:")
    print(f"   æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"   é€šè¿‡æµ‹è¯•æ•°: {summary['passed_tests']}")
    print(f"   å¤±è´¥æµ‹è¯•æ•°: {summary['failed_tests']}")
    print(f"   æˆåŠŸç‡: {summary['success_rate']:.1f}%")
    print(f"   åŸºå‡†æµ‹è¯•æ•°: {summary['benchmarks_run']}")
    print(f"   è¦†ç›–ç‡æŠ¥å‘Š: {'å·²ç”Ÿæˆ' if summary['coverage_generated'] else 'æœªç”Ÿæˆ'}")
    print(f"   å‘ç°é—®é¢˜æ•°: {len(report['issues'])}")
    print(f"   æ”¹è¿›å»ºè®®æ•°: {len(report['recommendations'])}")

if __name__ == "__main__":
    main()